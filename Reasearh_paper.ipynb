{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayat30/Research_paper/blob/main/Reasearh_paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "xr9q3Dz6d4dy",
        "outputId": "110e668c-e12a-40e0-e8fa-7082e147d399"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "papers = pd.read_csv('/content/NLPPP.csv')\n",
        "papers.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Title-name</th>\n",
              "      <th>Paper-text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1988</td>\n",
              "      <td>GUEST  EDITORIAL:GENETIC ALGORITHMS AND MACHIN...</td>\n",
              "      <td>M\\nachine learning and search\\ntechniques play...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019</td>\n",
              "      <td>LOCAL RELATION NETWORKS FOR IMAGE RECOGNITION</td>\n",
              "      <td>The convolution layer has been the dominant fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020</td>\n",
              "      <td>AN IMAGE ISWORTH16X16 WORDS:TRANSFORMERS FOR I...</td>\n",
              "      <td>While the Transformer architecture has become ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021</td>\n",
              "      <td>HANDLING BACKGROUND NOISE IN NEURAL SPEECH GEN...</td>\n",
              "      <td>Recent advances in neural-network based genera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021</td>\n",
              "      <td>GENERAL PERCEPTION WITH ITERATIVE ATTENTION</td>\n",
              "      <td>Biological systems understand the world by si-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Year  ...                                         Paper-text\n",
              "0  1988  ...  M\\nachine learning and search\\ntechniques play...\n",
              "1  2019  ...  The convolution layer has been the dominant fe...\n",
              "2  2020  ...  While the Transformer architecture has become ...\n",
              "3  2021  ...  Recent advances in neural-network based genera...\n",
              "4  2021  ...  Biological systems understand the world by si-...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oBVj-Bg9eE9J"
      },
      "source": [
        "# Group the papers by year\n",
        "groups = papers.groupby(by='Year')\n",
        "# Determine the size of each group\n",
        "counts = groups.size()\n",
        "# Visualise the counts as a bar plot\n",
        "import matplotlib.pyplot\n",
        "%matplotlib inline\n",
        "counts.plot(kind='bar');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tFjp6xSUKSE_"
      },
      "source": [
        "# Load the regular expression library\n",
        "import re\n",
        "# Print the titles of the first rows \n",
        "print(papers['Title-name'].head())\n",
        "# Remove punctuation\n",
        "papers['title_processed'] = papers['Title-name'].map(lambda x: re.sub('[,\\.!?]', ':', x))\n",
        "# Convert the titles to lowercase\n",
        "papers['title_processed'] = papers['title_processed'].map(lambda x: x.lower())\n",
        "# Print the processed titles of the first rows \n",
        "print(papers['title_processed'].head())\n",
        "papers.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZQVbVE5OeTg5"
      },
      "source": [
        "# Import the wordcloud library\n",
        "import wordcloud\n",
        "\n",
        "# Join the different processed titles together.\n",
        "long_string = ' '.join(papers['title_processed'])\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = wordcloud.WordCloud()\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "glkzFROGeaOY"
      },
      "source": [
        "# Load the library with the CountVectorizer method\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Helper function\n",
        "def plot_10_most_common_words(count_data, count_vectorizer):\n",
        "    import matplotlib.pyplot as plt\n",
        "    words = count_vectorizer.get_feature_names()\n",
        "    total_counts = np.zeros(len(words))\n",
        "    for t in count_data:\n",
        "        total_counts+=t.toarray()[0]\n",
        "    \n",
        "    count_dict = (zip(words, total_counts))\n",
        "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
        "    words = [w[0] for w in count_dict]\n",
        "    counts = [w[1] for w in count_dict]\n",
        "    x_pos = np.arange(len(words)) \n",
        "\n",
        "    plt.bar(x_pos, counts,align='center')\n",
        "    plt.xticks(x_pos, words, rotation=90) \n",
        "    plt.xlabel('words')\n",
        "    plt.ylabel('counts')\n",
        "    plt.title('10 most common words')\n",
        "    plt.show()\n",
        "\n",
        "# Initialise the count vectorizer with the English stop words\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the processed titles\n",
        "count_data = count_vectorizer.fit_transform(papers['title_processed'])\n",
        "\n",
        "# Visualise the 10 most common words\n",
        "plot_10_most_common_words(count_data, count_vectorizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lGUPurocenoo"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "\n",
        "# Load the LDA model from sk-learn\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        " \n",
        "# Helper function\n",
        "def print_topics(model, count_vectorizer, n_top_words):\n",
        "    words = count_vectorizer.get_feature_names()\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"\\nTopic #%d:\" % topic_idx)\n",
        "        print(\" \".join([words[i]\n",
        "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
        "        \n",
        "# Tweak the two parameters below (use int values below 15)\n",
        "number_topics = 10\n",
        "number_words = 10\n",
        "\n",
        "# Create and fit the LDA model\n",
        "lda = LDA(n_components=number_topics)\n",
        "lda.fit(count_data)\n",
        "\n",
        "# Print the topics found by the LDA model\n",
        "print(\"Topics found via LDA:\")\n",
        "print_topics(lda, count_vectorizer, number_words)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}